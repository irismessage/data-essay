{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "J_8Hl6RhR5Th",
   "metadata": {
    "id": "J_8Hl6RhR5Th"
   },
   "source": [
    "## Department of Computer Science, University of York\n",
    "### DATA: Introduction to Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833fc93b",
   "metadata": {
    "id": "833fc93b"
   },
   "source": [
    "## Task 1: Domain Analysis  (5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RbtRJd18asfN",
   "metadata": {
    "id": "RbtRJd18asfN"
   },
   "source": [
    "Given the business domain and the data overview presented (in the assessment paper), provide a brief description of\n",
    "\n",
    "* the business problem and its significance to the relevant sector;\n",
    "* the link between the business problem and the field of data science;\n",
    "* the main areas of investigation; and\n",
    "* potential ideas and solutions.\n",
    "\n",
    "\n",
    "**Word Limit:** 300 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LQ_p4Qitu334",
   "metadata": {
    "id": "LQ_p4Qitu334"
   },
   "source": [
    "**Write your answer here (text cell(s) to be used, as appropriate)**\n",
    "\n",
    "The questions at hand are of demographics, loan default prediction, and expansion area. Banks like this one make profit from interest on loans and credit cards[^1], so the objectives are to refuse loans that will default, give high-value customers reasons to continue doing business with the bank, and onboard new customers.\n",
    "\n",
    "We can leverage data science to load and quickly perform operations on huge amounts of figures and categorical data (e.g. over 100,000 rows of transactions), creating descriptions and visualisations of the data that people can understand. We can inform decision making by using techniques from statistics to summarise business operations and make predictions of the outcomes of providing credit.\n",
    "\n",
    "The investigations I have chosen to carry out will be the coverage of the customer base over different cities, and the way a customer's characteristics and prior usage of services relate to credit outcomes. Proposals will include strategies for predicting whether to authorise products, how to identify customers to target with perks, and potential geographic areas of expansion to focus on, using models like logistic regression and visualisation like bar charts.\n",
    "\n",
    "[^1]: https://www.investopedia.com/terms/c/commercialbank.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "G8kyLn-xXG9O",
   "metadata": {
    "id": "G8kyLn-xXG9O"
   },
   "outputs": [],
   "source": [
    "### Write your answer here (code cell(s) to be used, as appropriate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326757ae",
   "metadata": {
    "id": "326757ae"
   },
   "source": [
    "\n",
    "----\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f0a14c",
   "metadata": {
    "id": "f2f0a14c"
   },
   "source": [
    "## Task 2: Database Design (25 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dC2nOMbbU5AG",
   "metadata": {
    "id": "dC2nOMbbU5AG"
   },
   "source": [
    "Having understood the business domain, present a conceptual design in the form of an entity-relationship (ER) model that would be helpful in creating a database for the bank.\n",
    "\n",
    "The bank data currently exists in the form of a csv file called *BankRecords.csv*, provided on VLE (path given in page 5, assessment paper). This file has all the existing records. The table available in the csv file is unnormalised. The information about its different columns is given in Tables 1 and 2 (in the assessment paper).\n",
    "\n",
    "Following the standard principles of database normalisation, normalise the given table (*BankRecords.csv*) to a database schema that has minimum redundancies. Then, using the designed schema, create an SQLite database.\n",
    "\n",
    "Your answer should include the SQL statements needed to accomplish this step. Your submission should also include the created SQLite database file.\n",
    "\n",
    "Your answer should clearly cover the following:\n",
    "* Any assumptions you are making about the given scenario;\n",
    "* The designated keys, existing relationships, and identified functional dependencies;\n",
    "* The steps followed and justifications for the decisions made.\n",
    "\n",
    "**World Limit**: 500 words. This limit applies only to the explanations. There is no limit on any associated code/SQL statements or figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43203df7",
   "metadata": {
    "id": "43203df7"
   },
   "source": [
    "**Write your answer here (text cell(s) to be used, as appropriate)**\n",
    "\n",
    "# Normal form\n",
    "\n",
    "The data is already in first normal form as there are no composite values, so we can continue to 2NF.\n",
    "\n",
    "This table is clearly not in second normal form. Loan date is dependent on loan ID, name is dependent on client ID, etc. etc. There are a bunch of values that should be primary keys all in the same table. I split up the dataset into eight separate tables intuitively based on the \"Data Overview\" and \"Description of columns\" from the specification. Each unique ID gets its own table.\n",
    "\n",
    "Now in 2NF, I noticed noticed a transitive dependency - I had included dispID, clientID, and accountID together in their relevant tables. I realised clientID and accountID are both just dependent on dispID. Therefore I removed them from all the tables except Dispositon. Finally, we are in third normal form and finished.\n",
    "\n",
    "I briefly thought there was a transitive dependency from \"region\" to \"noAreas\" because the data overview said noAreas was the number of *cities* in the *region*, and was repeated for each *city*, but the inconsistent noAreas for each region and the mismatch of actual city count implies this is an error in the data overview.\n",
    "\n",
    "Reference:\n",
    "- DATA-Lecture4-DatabaseNormalisationAndSQL.pdf\n",
    "- https://en.wikipedia.org/wiki/Database_normalization\n",
    "\n",
    "# ERM\n",
    "For the ERM diagram, see `DATA Essay.jpg`.\n",
    "\n",
    "![ERM Diagram](DATA%20Essay.jpg)\n",
    "\n",
    "Reference:\n",
    "- DATA-Lecture3-RelationalDatabases.pdf\n",
    "- DATA-Practical3Part2 - Mapping ER model to tables - Solution.pdf\n",
    "- https://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5988039d-a777-44aa-93c3-e05b282f97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33ca09c",
   "metadata": {
    "id": "a33ca09c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting database\n",
      "Deleted database\n",
      "Creating database\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "### Write your answer here (code cell(s) to be used, as appropriate)\n",
    "\n",
    "# the following classifier fields could be made integers:\n",
    "#   Account\n",
    "#     statementFrequency\n",
    "#   Loan\n",
    "#     loanStatus\n",
    "#   StandingOrder\n",
    "#     NOT paymentType\n",
    "#   BankTransaction\n",
    "#     transType\n",
    "#     operation\n",
    "#     NOT transDetail\n",
    "#   Disposition\n",
    "#     dispType\n",
    "#   CreditCard\n",
    "#     cardType\n",
    "\n",
    "sql_script_path = Path(\"create_db.sqlite3\")\n",
    "sqlf = Path(\"BankRecords.db\")\n",
    "\n",
    "\n",
    "def create_db():\n",
    "    sql_create = sql_script_path.read_text(encoding=\"utf-8\")\n",
    "    \n",
    "    print(\"Deleting database\")\n",
    "    try:\n",
    "        sqlf.unlink()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Database didn't exist\")\n",
    "    else:\n",
    "        print(\"Deleted database\")\n",
    "    \n",
    "    print(\"Creating database\")\n",
    "    con = sqlite3.connect(sqlf)\n",
    "    with con:\n",
    "        con.executescript(sql_create)\n",
    "    con.close()\n",
    "    print(\"Done\")\n",
    "\n",
    "\n",
    "create_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b603743e-4bce-4109-893c-32f15f5bc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0690ff0d-59b5-4578-8fb1-0f0f6b1d9b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading csv\n",
      "CPU times: user 6.54 s, sys: 4.93 s, total: 11.5 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load dataframe\n",
    "DATE_FORMAT_CSV = \"%y%m%d\"\n",
    "\n",
    "csvf = Path(\"BankRecords.csv\")\n",
    "\n",
    "print(\"Loading csv\")\n",
    "\n",
    "# could manually convert to datetime after reading\n",
    "# to set the unit to something better for dates than ns\n",
    "# but this is fine\n",
    "date_columns = [\n",
    "    \"creation_date\",\n",
    "    \"loan_date\",\n",
    "    \"trans_date\",\n",
    "    \"card_issued\",\n",
    "]\n",
    "date_formats = {c: DATE_FORMAT_CSV for c in date_columns}\n",
    "date_formats[\"card_issued\"] = f\"{DATE_FORMAT_CSV} 00:00:00\"  # they lied about this one\n",
    "\n",
    "df = pd.read_csv(\n",
    "    csvf,\n",
    "    parse_dates=date_columns,\n",
    "    date_format=date_formats,\n",
    "    low_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f888f528-bf8a-4be8-bf0b-c4802946302e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>frequency</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>loan_id</th>\n",
       "      <th>loan_date</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>loan_duration</th>\n",
       "      <th>loan_payments</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>order_id</th>\n",
       "      <th>...</th>\n",
       "      <th>a7</th>\n",
       "      <th>a8</th>\n",
       "      <th>a9</th>\n",
       "      <th>a10</th>\n",
       "      <th>a11</th>\n",
       "      <th>a12</th>\n",
       "      <th>a13</th>\n",
       "      <th>a14</th>\n",
       "      <th>a15</th>\n",
       "      <th>a16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>1995-03-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29401.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>65.3</td>\n",
       "      <td>8968</td>\n",
       "      <td>2.83</td>\n",
       "      <td>3.35</td>\n",
       "      <td>131</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>1995-03-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29401.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>65.3</td>\n",
       "      <td>8968</td>\n",
       "      <td>2.83</td>\n",
       "      <td>3.35</td>\n",
       "      <td>131</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>1995-03-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29401.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>65.3</td>\n",
       "      <td>8968</td>\n",
       "      <td>2.83</td>\n",
       "      <td>3.35</td>\n",
       "      <td>131</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>1995-03-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29401.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>65.3</td>\n",
       "      <td>8968</td>\n",
       "      <td>2.83</td>\n",
       "      <td>3.35</td>\n",
       "      <td>131</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>1995-03-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29401.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>65.3</td>\n",
       "      <td>8968</td>\n",
       "      <td>2.83</td>\n",
       "      <td>3.35</td>\n",
       "      <td>131</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_id frequency creation_date  loan_id loan_date  loan_amount  \\\n",
       "0           1   Monthly    1995-03-24      NaN       NaT          NaN   \n",
       "1           1   Monthly    1995-03-24      NaN       NaT          NaN   \n",
       "2           1   Monthly    1995-03-24      NaN       NaT          NaN   \n",
       "3           1   Monthly    1995-03-24      NaN       NaT          NaN   \n",
       "4           1   Monthly    1995-03-24      NaN       NaT          NaN   \n",
       "\n",
       "   loan_duration  loan_payments loan_status  order_id  ... a7  a8  a9   a10  \\\n",
       "0            NaN            NaN         NaN   29401.0  ...  2   1   4  65.3   \n",
       "1            NaN            NaN         NaN   29401.0  ...  2   1   4  65.3   \n",
       "2            NaN            NaN         NaN   29401.0  ...  2   1   4  65.3   \n",
       "3            NaN            NaN         NaN   29401.0  ...  2   1   4  65.3   \n",
       "4            NaN            NaN         NaN   29401.0  ...  2   1   4  65.3   \n",
       "\n",
       "    a11   a12   a13  a14     a15   a16  \n",
       "0  8968  2.83  3.35  131  1740.0  1910  \n",
       "1  8968  2.83  3.35  131  1740.0  1910  \n",
       "2  8968  2.83  3.35  131  1740.0  1910  \n",
       "3  8968  2.83  3.35  131  1740.0  1910  \n",
       "4  8968  2.83  3.35  131  1740.0  1910  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6735fb26-42d7-4621-b5ff-d92dd5778e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2243458, 46)\n",
      "account_id                  int64\n",
      "frequency                  object\n",
      "creation_date      datetime64[ns]\n",
      "loan_id                   float64\n",
      "loan_date          datetime64[ns]\n",
      "loan_amount               float64\n",
      "loan_duration             float64\n",
      "loan_payments             float64\n",
      "loan_status                object\n",
      "order_id                  float64\n",
      "bank_to                    object\n",
      "account_to                float64\n",
      "order_amount              float64\n",
      "payment_type               object\n",
      "trans_id                    int64\n",
      "trans_date         datetime64[ns]\n",
      "trans_type                 object\n",
      "operation                  object\n",
      "trans_amount              float64\n",
      "balance                   float64\n",
      "trans_detail               object\n",
      "partner_bank               object\n",
      "partner_account           float64\n",
      "disp_id                     int64\n",
      "client_id                   int64\n",
      "disp_type                  object\n",
      "card_id                   float64\n",
      "card_type                  object\n",
      "card_issued        datetime64[ns]\n",
      "birth_number                int64\n",
      "a1                          int64\n",
      "a2                         object\n",
      "a3                         object\n",
      "a4                          int64\n",
      "a5                          int64\n",
      "a6                          int64\n",
      "a7                          int64\n",
      "a8                          int64\n",
      "a9                          int64\n",
      "a10                       float64\n",
      "a11                         int64\n",
      "a12                       float64\n",
      "a13                       float64\n",
      "a14                         int64\n",
      "a15                       float64\n",
      "a16                         int64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['account_id', 'frequency', 'creation_date', 'loan_id', 'loan_date',\n",
       "       'loan_amount', 'loan_duration', 'loan_payments', 'loan_status',\n",
       "       'order_id', 'bank_to', 'account_to', 'order_amount', 'payment_type',\n",
       "       'trans_id', 'trans_date', 'trans_type', 'operation', 'trans_amount',\n",
       "       'balance', 'trans_detail', 'partner_bank', 'partner_account', 'disp_id',\n",
       "       'client_id', 'disp_type', 'card_id', 'card_type', 'card_issued',\n",
       "       'birth_number', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9',\n",
       "       'a10', 'a11', 'a12', 'a13', 'a14', 'a15', 'a16'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that some columns we would expect to be int are float instead\n",
    "# because int can't be NaN but float can\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bce34d56-5161-47db-b15c-b5eb961f9b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [account_id, frequency, creation_date, loan_id, loan_date, loan_amount, loan_duration, loan_payments, loan_status, order_id, bank_to, account_to, order_amount, payment_type, trans_id, trans_date, trans_type, operation, trans_amount, balance, trans_detail, partner_bank, partner_account, disp_id, client_id, disp_type, card_id, card_type, card_issued, birth_number, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "x = \"loan_amount\"\n",
    "print(df[(df[x] % 1 != 0) & df[x].notna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f889f13-9d51-44dc-8d94-edf28b87ec4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         account_id frequency creation_date  loan_id  loan_date  loan_amount  \\\n",
      "2241480       11359   Monthly    1994-10-01   7305.0 1996-08-06      54024.0   \n",
      "2241481       11359   Monthly    1994-10-01   7305.0 1996-08-06      54024.0   \n",
      "2241482       11359   Monthly    1994-10-01   7305.0 1996-08-06      54024.0   \n",
      "2241483       11359   Monthly    1994-10-01   7305.0 1996-08-06      54024.0   \n",
      "2241484       11359   Monthly    1994-10-01   7305.0 1996-08-06      54024.0   \n",
      "\n",
      "         loan_duration  loan_payments loan_status  order_id bank_to  \\\n",
      "2241480           12.0         4502.0           A   46329.0      OP   \n",
      "2241481           12.0         4502.0           A   46329.0      OP   \n",
      "2241482           12.0         4502.0           A   46329.0      OP   \n",
      "2241483           12.0         4502.0           A   46329.0      OP   \n",
      "2241484           12.0         4502.0           A   46329.0      OP   \n",
      "\n",
      "         account_to  order_amount payment_type  trans_id trans_date  \\\n",
      "2241480  33080721.0        4518.0    Household   3423128 1998-12-11   \n",
      "2241481  33080721.0        4518.0    Household   3423093 1998-12-11   \n",
      "2241482  33080721.0        4518.0    Household   3422913 1998-12-14   \n",
      "2241483  33080721.0        4518.0    Household   3422973 1998-12-21   \n",
      "2241484  33080721.0        4518.0    Household   3529998 1998-12-31   \n",
      "\n",
      "         trans_type                     operation  trans_amount  balance  \\\n",
      "2241480  Withdrawal        Credit card withdrawal        1800.0  38550.8   \n",
      "2241481  Withdrawal    Remittance to another bank         254.0  38296.8   \n",
      "2241482      Credit  Collection from another bank       40521.0  78817.8   \n",
      "2241483  Withdrawal            Withdrawal in cash        3900.0  74917.8   \n",
      "2241484      Credit                           NaN         282.6  75200.4   \n",
      "\n",
      "              trans_detail partner_bank  partner_account  disp_id  client_id  \\\n",
      "2241480                NaN          NaN              0.0    13660      13968   \n",
      "2241481                NaN           AB       90330576.0    13660      13968   \n",
      "2241482                NaN           MN       87391823.0    13660      13968   \n",
      "2241483                NaN          NaN              0.0    13660      13968   \n",
      "2241484  Interest credited          NaN              NaN    13660      13968   \n",
      "\n",
      "        disp_type  card_id card_type card_issued  birth_number  a1        a2  \\\n",
      "2241480     OWNER   1247.0   classic  1995-06-13        680413  61  Swindon    \n",
      "2241481     OWNER   1247.0   classic  1995-06-13        680413  61  Swindon    \n",
      "2241482     OWNER   1247.0   classic  1995-06-13        680413  61  Swindon    \n",
      "2241483     OWNER   1247.0   classic  1995-06-13        680413  61  Swindon    \n",
      "2241484     OWNER   1247.0   classic  1995-06-13        680413  61  Swindon    \n",
      "\n",
      "                a3      a4   a5  a6  a7  a8  a9   a10   a11   a12   a13  a14  \\\n",
      "2241480  Wiltshire  117897  139  28   5   1   6  53.8  8814  4.76  5.74  107   \n",
      "2241481  Wiltshire  117897  139  28   5   1   6  53.8  8814  4.76  5.74  107   \n",
      "2241482  Wiltshire  117897  139  28   5   1   6  53.8  8814  4.76  5.74  107   \n",
      "2241483  Wiltshire  117897  139  28   5   1   6  53.8  8814  4.76  5.74  107   \n",
      "2241484  Wiltshire  117897  139  28   5   1   6  53.8  8814  4.76  5.74  107   \n",
      "\n",
      "            a15   a16  \n",
      "2241480  2112.0  2059  \n",
      "2241481  2112.0  2059  \n",
      "2241482  2112.0  2059  \n",
      "2241483  2112.0  2059  \n",
      "2241484  2112.0  2059  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", 100)\n",
    "print(df[df[\"trans_id\"].notna() & df[\"card_id\"].notna()].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06acb02a-0b56-4f02-81a1-103fedc6d953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creation_date 0\n",
      "loan_date 0\n",
      "trans_date 0\n",
      "card_issued 0\n"
     ]
    }
   ],
   "source": [
    "# since a 2-digit year is ambiguous we have to check this works fine\n",
    "# python's datetime pivot year is 1970\n",
    "# so we can verify all the years are between 70 and 99 like so:\n",
    "for c in date_columns:\n",
    "    print(c, sum(df[c] > pd.Timestamp(year=2000, month=1, day=1)))\n",
    "\n",
    "# therefore, in the special parsing we do for birth_number\n",
    "# we can assume all dates lie in the decade 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88cbae1f-f851-405a-b38b-8299e25883ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting database\n",
      "Deleted database\n",
      "Creating database\n",
      "Done\n",
      "Inserting data\n",
      "Inserting data complete\n",
      "CPU times: user 8.62 s, sys: 310 ms, total: 8.93 s\n",
      "Wall time: 9.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ISO 8601 date\n",
    "# https://www.sqlite.org/lang_datefunc.html\n",
    "DATE_FORMAT_SQL = \"%Y-%m-%d\"\n",
    "\n",
    "\n",
    "# reset db for testing\n",
    "create_db()\n",
    "\n",
    "\n",
    "def filter_df(columns: list[str]):\n",
    "    return df.filter(columns).groupby(columns[0]).first()\n",
    "\n",
    "\n",
    "print(\"Inserting data\")\n",
    "con = sqlite3.connect(sqlf)\n",
    "cur = con.cursor()\n",
    "\n",
    "accounts = filter_df(\n",
    "    [\"account_id\", \"frequency\", \"creation_date\"]\n",
    ")\n",
    "# no inplace option for this 3':\n",
    "accounts[\"creation_date\"] = accounts[\"creation_date\"].dt.strftime(DATE_FORMAT_SQL)\n",
    "cur.executemany(\n",
    "    \"INSERT INTO Account VALUES (?, ?, ?);\",\n",
    "    accounts.itertuples()\n",
    ")\n",
    "\n",
    "loans = filter_df(\n",
    "    [\"loan_id\", \"disp_id\", \"loan_date\", \"loan_amount\", \"loan_duration\", \"loan_payments\", \"loan_status\"]\n",
    ")\n",
    "loans[\"loan_date\"] = loans[\"loan_date\"].dt.strftime(DATE_FORMAT_SQL)\n",
    "cur.executemany(\n",
    "    \"INSERT INTO Loan VALUES (?, ?, ?, ?, ?, ?, ?);\",\n",
    "    loans.itertuples()\n",
    ")\n",
    "\n",
    "standing_orders = filter_df(\n",
    "    [\"order_id\", \"disp_id\", \"bank_to\", \"account_to\", \"order_amount\", \"payment_type\"]\n",
    ")\n",
    "cur.executemany(\n",
    "    \"INSERT INTO StandingOrder VALUES (?, ?, ?, ?, ?, ?);\",\n",
    "    standing_orders.itertuples()\n",
    ")\n",
    "\n",
    "transactions = filter_df(\n",
    "    [\n",
    "        \"trans_id\", \"disp_id\",\n",
    "        \"trans_date\", \"trans_type\", \"operation\", \"trans_amount\", \"balance\", \"trans_detail\", \"partner_bank\", \"partner_account\",\n",
    "    ]\n",
    ")\n",
    "transactions[\"trans_date\"] = transactions[\"trans_date\"].dt.strftime(DATE_FORMAT_SQL)\n",
    "cur.executemany(\n",
    "    \"INSERT INTO BankTransaction VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\",\n",
    "    transactions.itertuples()\n",
    ")\n",
    "\n",
    "clients = filter_df(\n",
    "    [\"client_id\", \"a1\", \"birth_number\"]\n",
    ")\n",
    "# separating birth number into relevant columns\n",
    "# could also do this as string operations\n",
    "# bool mask of all rows with MM+50\n",
    "clients[\"gender\"] = ((clients[\"birth_number\"] // 100) % 100) > 12\n",
    "# remove offset\n",
    "# 100(x + 50) = 100x + 5000\n",
    "clients.loc[clients[\"gender\"], \"birth_number\"] -= 5000\n",
    "# parse date\n",
    "clients[\"birth_number\"] += 19000000  # see above cell\n",
    "clients[\"birth_number\"] = pd.to_datetime(clients[\"birth_number\"], format=\"%Y%m%d\")\n",
    "clients[\"birth_number\"] = clients[\"birth_number\"].dt.strftime(DATE_FORMAT_SQL)\n",
    "cur.executemany(\n",
    "    \"INSERT INTO Client VALUES (?, ?, ?, ?);\",\n",
    "    clients.itertuples()\n",
    ")\n",
    "\n",
    "dispositions = filter_df(\n",
    "    [\"disp_id\", \"account_id\", \"client_id\", \"disp_type\"]\n",
    ")\n",
    "# dispo elysium\n",
    "cur.executemany(\n",
    "    \"INSERT INTO Disposition VALUES (?, ?, ?, ?);\",\n",
    "    dispositions.itertuples()\n",
    ")\n",
    "\n",
    "credit_cars = filter_df(\n",
    "    [\"card_id\", \"disp_id\", \"card_type\", \"card_issued\"]\n",
    ")\n",
    "credit_cars[\"card_issued\"] = credit_cars[\"card_issued\"].dt.strftime(DATE_FORMAT_SQL)\n",
    "cur.executemany(\n",
    "    \"INSERT INTO CreditCard VALUES (?, ?, ?, ?);\",\n",
    "    credit_cars.itertuples()\n",
    ")\n",
    "\n",
    "cities = filter_df(\n",
    "    [f\"a{i}\" for i in range(1, 16+1)]\n",
    ")\n",
    "cur.executemany(\n",
    "    \"INSERT INTO City VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\",\n",
    "    cities.itertuples()\n",
    ")\n",
    "\n",
    "cur.close()\n",
    "con.commit()\n",
    "con.close()\n",
    "print(\"Inserting data complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56000f9f",
   "metadata": {
    "id": "56000f9f",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "----\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4f6c6",
   "metadata": {
    "id": "bbd4f6c6"
   },
   "source": [
    "## Task 3: Research Design (25 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4db00",
   "metadata": {
    "id": "26d4db00"
   },
   "source": [
    "Using the database designed in Task 2, design and implement **five** potential modelling solutions to achieve the aim of the Data Intelligence team. You need to provide clear justifications about the techniques selected in the context of the 'problem in hand'. Your design must consist of a combination of inferential statistics, supervised learning algorithms, and unsupervised learning algorithms, and include **at least one** of those techniques. Finally, your modelling solutions should be of sufficient complexity, combining information from multiple tables from the database built in Task 2, as appropriate. Your answer should clearly show the queries made to the database. If amendments are made to the database, the commands should be clearly included in your answer.\n",
    "\n",
    "Your answer should clearly cover the following:\n",
    "* Any assumptions you are making about the given scenario;\n",
    "* Any data processing and data integrity steps you would undertake to make the data fit for purpose;\n",
    "* Which technique(s) you would apply for each solution and why;\n",
    "* An evaluation of the techniques applied in terms of the accuracy of their results (or any other suitable evaluation measure);\n",
    "* Algorithmic parameters should be adequately stated and discussed;\n",
    "* A discussion of ethical considerations arising from the solutions selected.\n",
    "\n",
    "**World Limit**: 500 words. This limit applies only to the explanations. There is no limit on any associated code or figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VVfHwzLCX_NX",
   "metadata": {
    "id": "VVfHwzLCX_NX"
   },
   "source": [
    "**Write your answer here (text cell(s) to be used, as appropriate)**\n",
    "\n",
    "techniques\n",
    "1. inferential - standard distribution and hypothesis testing\n",
    "2. supervised - linear regression (simple, multiple, polynomial)\n",
    "    - https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html\n",
    "3. supervised - decision tree (classification)\n",
    "4. unsupervised - clustering (classification)\n",
    "\n",
    "suggestions from paper\n",
    "- customer risk assessment\n",
    "- customer retention scheme\n",
    "\n",
    "idea 1. logistic regression on loan A/B with city characteristics\n",
    "try clustering and see if anythign intersting happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9cc2086",
   "metadata": {
    "id": "a9cc2086"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                             5369\n",
       "mean     1953-09-12 09:32:21.143602176\n",
       "min                1911-08-20 00:00:00\n",
       "25%                1940-11-25 00:00:00\n",
       "50%                1954-05-06 00:00:00\n",
       "75%                1968-06-09 00:00:00\n",
       "max                1987-09-27 00:00:00\n",
       "Name: birthDate, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Write your answer here (code cell(s) to be used, as appropriate)\n",
    "\n",
    "con = sqlite3.connect(sqlf)\n",
    "dfc = pd.read_sql_query(\"SELECT * FROM Client;\", con, parse_dates=[\"birthDate\"])\n",
    "con.close()\n",
    "\n",
    "\n",
    "dfc[\"birthDate\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888817af-8f3f-4abb-b30a-21afd1109829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok let's try this\n",
    "# todo include whether they paid back loans in the past\n",
    "# todo include account creation date\n",
    "sql_read_loan_data = \"\"\"\n",
    "SELECT Loan.loanStatus, Loan.loanAmount, Client.birthDate, \n",
    "City.inhabitants, City.ratioUrban, City.avgSalary, City.unemployment1995, City.unemployment1996, City.entrepeneurs, City.crimes1995, City.crimes1996\n",
    "FROM (((Loan\n",
    "JOIN Disposition ON Loan.dispID = Disposition.dispID)\n",
    "JOIN Client ON Disposition.clientID = Client.clientID)\n",
    "JOIN City ON Client.cityID = City.cityID)\n",
    "WHERE loanStatus IN ('A', 'B');\n",
    "\"\"\"\n",
    "con = sqlite3.connect(sqlf)\n",
    "df_risk = pd.read_sql_query(sql_read_loan_data, con, parse_dates=[\"birthDate\"])\n",
    "con.close()\n",
    "df_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7bfefcd9-c3a1-4a01-88b9-b68717f7e93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "229    0\n",
       "230    0\n",
       "231    0\n",
       "232    0\n",
       "233    0\n",
       "Length: 232, dtype: int8"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_risk.dropna(inplace=True)  # one or two empty unemployment fields\n",
    "y = df_risk[\"loanStatus\"].astype(\"category\")\n",
    "y = y.cat.codes  # 0 = paid off, 1 = not paid\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f9b89d93-5471-4190-8497-3d41be995f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6eba81a3-7098-4791-afe8-1f0f453e2c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo train test split\n",
    "# todo add bank balance\n",
    "\n",
    "# idea: filter transacitons by credit card type. for customer retention stuff\n",
    "\n",
    "# well, keep slogging at it and something will come to me even if I can't see it now\n",
    "\n",
    "X = df_risk.drop(columns=[\"loanStatus\"])\n",
    "X[\"birthDate\"] = X[\"birthDate\"].dt.year\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0794c-0726-455b-89ef-f21fc684d53b",
   "metadata": {
    "id": "d2dc2a18"
   },
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f808a0fa",
   "metadata": {
    "id": "f808a0fa"
   },
   "source": [
    "## Task 4: Experimental Results and Analysis (25 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f250b",
   "metadata": {
    "id": "c71f250b"
   },
   "source": [
    "Given the **five** modelling solutions implemented above, analyse, discuss and present your findings to the key stakeholders of the bank.\n",
    "\n",
    "Your answer should clearly cover the following:\n",
    "* Present your findings in a clear and concise manner;\n",
    "* Discuss your results in the context of the selected solution;\n",
    "* Discuss how these results can help the bank in performing customer risk assessment and establishing customer retention strategies;\n",
    "* Present the limitations (if any) of your solutions in a clear and concise manner.\n",
    "\n",
    "**World Limit**: 500 words. This limit applies only to the explanations. There is no limit on any associated code or figures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sXb02r__Ykgr",
   "metadata": {
    "id": "sXb02r__Ykgr"
   },
   "source": [
    "**Write your answer here (text cell(s) to be used, as appropriate)**\n",
    "![PowerPoint®](think.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521811e9",
   "metadata": {
    "id": "521811e9"
   },
   "outputs": [],
   "source": [
    "### Write your answer here (code cell(s) to be used, as appropriate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc2a18",
   "metadata": {
    "id": "d2dc2a18"
   },
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728dd77f",
   "metadata": {
    "id": "728dd77f"
   },
   "source": [
    "## Task 5: Conclusion (10 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be71ded",
   "metadata": {
    "id": "5be71ded"
   },
   "source": [
    "Given the insights derived from Tasks 1-4, provide a conclusion that clearly covers the following:\n",
    "* A summary of the main points;\n",
    "* A discussion of the significance of your results;\n",
    "* Any recommendation(s) resulting from your analysis;\n",
    "* Any overall ethical considerations arising from the data analysis of this business domain.\n",
    "\n",
    "**World Limit**: 300 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OmyhUuiqZc70",
   "metadata": {
    "id": "OmyhUuiqZc70"
   },
   "source": [
    "**Write your answer here (text cell(s) to be used, as appropriate)**\n",
    "\n",
    "In summary, discombobulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334be1b5",
   "metadata": {
    "id": "334be1b5"
   },
   "outputs": [],
   "source": [
    "### Write your answer here (code cell(s) to be used, as appropriate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd1577",
   "metadata": {
    "id": "a1bd1577"
   },
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce07aa2",
   "metadata": {
    "id": "dce07aa2"
   },
   "source": [
    "## Overall Academic Quality (10 Marks)\n",
    "10 marks are allocated for the clarity and cohesiveness of your answers (both text and code) across all tasks with appropriate, relevant and effective analysis and presentation of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9440ef2b",
   "metadata": {
    "id": "9440ef2b"
   },
   "source": [
    "## Deliverables\n",
    "\n",
    "You should submit the following to the submission point on the teaching portal:\n",
    "\n",
    "1. the SQLite database produced in Task 2;\n",
    "2. the completed Jupyter notebook (both .ipynb and HTML files) that also includes the SQL statements (Task 2), the research design and its implementation (Task 3), and the analysis and presentation of your results (Task 4);\n",
    "3. any figures or diagrams that are included in your answers in the Jupyter notebook.\n",
    "\n",
    "For each task where text is required, we have provided guidelines above on the suggested word counts. Exceeding the word count will result in any work beyond the word count being disregarded when assessing."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
